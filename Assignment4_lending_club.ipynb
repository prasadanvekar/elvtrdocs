{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSGxJUD1Wx3MU1QBm1AFwo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prasadanvekar/elvtrdocs/blob/main/Assignment4_lending_club.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################################\n",
        "# Assignment#4: Lending Club.                                                                 #\n",
        "# Objective : apply essential data cleaning and transformation techniques to prepare the data #\n",
        "# for further analysis and modeling.                                                          #\n",
        "# Author: Prasad S Anvekar                                                                    #\n",
        "# Date: 28 Dec 2023                                                                           #\n",
        "###############################################################################################\n",
        "\n",
        "# Importing the required libraries\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the Data Path where the file are located.\n",
        "\n",
        "DATA_PATH = \"accepted_2007_to_2018Q4.csv\"\n",
        "\n",
        "# Error Hanlding for Data Path and File not found scenarios.\n",
        "\n",
        "if os.path.exists(DATA_PATH):\n",
        "    # Read from file if we've already downloaded the data.\n",
        "    with open(DATA_PATH) as f:\n",
        "        data = pd.read_csv('accepted_2007_to_2018Q4.csv', low_memory=False)\n",
        "else:\n",
        "     print(\"File not found!\")\n",
        "\n",
        "# Data exploration & Preprocessing\n",
        "# understanding the data\n",
        "\n",
        "# Get the number of columns\n",
        "num_columns = data.shape[1]\n",
        "\n",
        "# Display column names\n",
        "column_names = data.columns.tolist()\n",
        "\n",
        "print(f\"Number of Columns: {num_columns}\")\n",
        "print(f\"Column Names: {column_names}\")\n",
        "\n",
        "# Original Data:\n",
        "# To view the first 5 rows of the datasets\n",
        "print('Original Data - Display first few(5) rows of dataset:')\n",
        "print(data.head())\n",
        "\n",
        "# importing selected columns for this assignments\n",
        "# Specify the columns you want to import\n",
        "numeric_columns = ['id','loan_amnt', 'int_rate', 'annual_inc','settlement_amount','settlement_term','settlement_percentage']\n",
        "character_columns = ['term', 'grade', 'emp_length','verification_status','loan_status','settlement_status','disbursement_method', 'debt_settlement_flag']\n",
        "\n",
        "# Create a heatmap to visualize the correlation between numeric values\n",
        "correlation_matrix = data.corr()\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(correlation_matrix,annot=True,cmap='coolwarm',fmt='.2f')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Read the CSV file with selected columns\n",
        "selected_columns = numeric_columns + character_columns\n",
        "df = pd.read_csv(DATA_PATH, usecols=selected_columns)\n",
        "\n",
        "# To view the first 5 rows of the datasets\n",
        "print('Inspect Data - Display first few(5) rows of dataset:')\n",
        "print(df.head())\n",
        "\n",
        "# To view the last 5 rows of the datasets\n",
        "print('Inspect Data - Display last few(5) rows of dataset:')\n",
        "print(df.tail())\n",
        "\n",
        "# Check for any missing values\n",
        "print('Inspect any missing values:')\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# To view the column details of the datasets. To choose the right target variable.\n",
        "print('Inspect Data - Describe the column in the dataset:')\n",
        "print(df.describe())\n",
        "\n",
        "# to check the data types\n",
        "print('Inspect Data - check data types:')\n",
        "print(df.dtypes)\n",
        "\n",
        "# Handling the Missing Values:\n",
        "# for this assignment will be choosing 3 numeric - loan_amnt,int_rate,settlement_amount\n",
        "# and 3 character fields - settlement_status,debt_settlement_flag,disbursement_method\n",
        "# Replacing missing values with a mean (loan_amt, int_rate)\n",
        "imputer = SimpleImputer(missing_values=np.nan,strategy='mean')\n",
        "df[['loan_amnt', 'int_rate']] = imputer.fit_transform(df[['loan_amnt', 'int_rate']])\n",
        "\n",
        "# Replacing missing values with a constant\n",
        "imputer_1 = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
        "df[['settlement_amount','settlement_term','settlement_percentage']] = imputer_1.fit_transform(df[['settlement_amount','settlement_term','settlement_percentage']])\n",
        "\n",
        "# Replacing missing values with a constant\n",
        "imputer_2 = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='N')\n",
        "df[['settlement_status','debt_settlement_flag']] = imputer_2.fit_transform(df[['settlement_status','debt_settlement_flag']])\n",
        "\n",
        "# Replacing missing values with a constant\n",
        "#imputer_3 = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=None)\n",
        "#data[['disbursement_method']] = imputer_3.fit_transform(data[['disbursement_method']])\n",
        "\n",
        "print(\"Missing data replaced with constant & mean\")\n",
        "print(df.head())\n",
        "\n",
        "# Check for any missing values\n",
        "#print('Inspect any missing values post data imputation:')\n",
        "#print(df.isnull().sum())\n",
        "\n",
        "# dropping the rows with null values\n",
        "#df.dropna(subset=[''], inplace=True)\n",
        "\n",
        "# Encoding categorical variables (grade, loan_status)\n",
        "#le = LabelEncoder()\n",
        "#df['grade'] = le.fit_transform(df['grade'])\n",
        "#df['loan_status'] = le.fit_transform(df['loan_status'])\n",
        "\n",
        "#print(\"one-hot Encoding categorical data: \")\n",
        "#print(df)\n",
        "\n",
        "# Scaling numerical features (Annual Income) using StandardScaler:\n",
        "#scaler = StandardScaler()\n",
        "#df[['annual_inc']] = scaler.fit_transform(df[['annual_inc']])\n",
        "\n",
        "#Display the preprocessed and cleansed data\n",
        "#print(\"Scaling numnerical features using StandardScaler: \")\n",
        "#print(df)\n",
        "\n",
        "# write the transformed data to a new csv file for further processing & analysis.\n",
        "#df.to_csv('accepted_2007_to_2018Q4_modified.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "B-Nx8M02HunX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "ff3ef432-5703-4519-9817-db478f0b178a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File not found!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b5040429fd92>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Get the number of columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mnum_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Display column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    }
  ]
}